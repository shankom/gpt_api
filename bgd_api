from flask import Flask, request, jsonify
import openai
from pinecone import Pinecone

app = Flask(__name__)

# Initialize Pinecone client
pc = Pinecone(api_key="your_pinecone_api_key")
index = pc.Index("your_index_name")

# Set your OpenAI API key here
openai.api_key = "your_openai_api_key"

@app.route('/generate_embeddings', methods=['POST'])
def generate_embeddings():
    content = request.json
    query = content['query']
    response = openai.Embedding.create(
        model="text-embedding-ada-002",
        input=query
    )
    embedding_vector = response['data'][0]['embedding']
    return jsonify(embedding_vector)

@app.route('/get_relevant_kb_entries', methods=['POST'])
def get_relevant_kb_entries():
    content = request.json
    query = content['query']
    query_vector = generate_embeddings(query)
    response = index.query(vector=query_vector, top_k=3,namespace="",include_values=True,include_metadata=True)
    results = []
    for match in response.matches:
        meta = match.metadata if 'metadata' in match else None
        if meta and 'text' in meta:
            results.append(meta['text'])
    return jsonify(results if results else ["No relevant entries found or missing metadata."])

@app.route('/get_chat_completion', methods=['POST'])
def get_chat_completion():
    content = request.json
    user_query = content['user_query']
    relevant_kb_entries = get_relevant_kb_entries(user_query)
    context = "\n".join(relevant_kb_entries)
    full_context = f"The following information from the knowledge base is relevant:\n{context}\n\n{user_query}"
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": full_context}
        ]
    )
    return jsonify(response.choices[0].message['content'] if response.choices else "No response generated.")

if __name__ == '__main__':
    app.run(debug=True)
